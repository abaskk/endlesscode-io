{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LeetCode Rating Predictor\n",
                "\n",
                "Predict missing difficulty ratings for LeetCode problems using Gradient Boosting Regression.\n",
                "\n",
                "**Approach:**\n",
                "1. Baseline: TF-IDF + Tags + Metadata → GradientBoostingRegressor\n",
                "2. Enhanced: Add SentenceTransformer embeddings\n",
                "3. Compare performance and generate predictions\n",
                "\n",
                "**Datasets:**\n",
                "- `zerotrac.json`: 2,405 problems with ratings (PRIMARY)\n",
                "- `lcid.json`: 3,807 problems with metadata (tags, acRate)\n",
                "- `merged_problems.json`: 2,913 problems with descriptions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "print(\"Imports complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load zerotrac (has ratings)\n",
                "with open('source/zerotrac.json', 'r') as f:\n",
                "    zerotrac_data = json.load(f)\n",
                "df_zerotrac = pd.DataFrame(zerotrac_data)\n",
                "print(f\"Zerotrac: {len(df_zerotrac)} problems with ratings\")\n",
                "\n",
                "# Load lcid (has metadata)\n",
                "with open('source/lcid.json', 'r') as f:\n",
                "    lcid_data = json.load(f)\n",
                "# Convert dict to DataFrame\n",
                "lcid_rows = [{'problem_id': k, **v} for k, v in lcid_data.items()]\n",
                "df_lcid = pd.DataFrame(lcid_rows)\n",
                "print(f\"LCID: {len(df_lcid)} problems with metadata\")\n",
                "\n",
                "# Load merged_problems (has descriptions)\n",
                "with open('source/merged_problems.json', 'r') as f:\n",
                "    merged_data = json.load(f)\n",
                "df_problems = pd.DataFrame(merged_data['questions'])\n",
                "print(f\"Merged Problems: {len(df_problems)} problems with descriptions\")\n",
                "\n",
                "print(\"\\nData loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Merging & Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize TitleSlug for merging\n",
                "df_zerotrac['slug'] = df_zerotrac['TitleSlug'].str.lower().str.strip()\n",
                "df_lcid['slug'] = df_lcid['titleSlug'].str.lower().str.strip()\n",
                "df_problems['slug'] = df_problems['problem_slug'].str.lower().str.strip()\n",
                "\n",
                "# Merge step 1: zerotrac + lcid\n",
                "df_merged = df_zerotrac.merge(df_lcid, on='slug', how='outer', suffixes=('_zt', '_lcid'))\n",
                "print(f\"After zerotrac + lcid merge: {len(df_merged)} problems\")\n",
                "\n",
                "# Merge step 2: + merged_problems\n",
                "df_merged = df_merged.merge(df_problems, on='slug', how='outer')\n",
                "print(f\"After adding descriptions: {len(df_merged)} problems\")\n",
                "\n",
                "# Check for duplicates\n",
                "assert df_merged['slug'].duplicated().sum() == 0, \"Found duplicate slugs!\"\n",
                "print(\"No duplicate slugs\")\n",
                "\n",
                "# Analyze rating coverage\n",
                "has_rating = df_merged['Rating'].notna()\n",
                "has_description = df_merged['description'].notna()\n",
                "usable_for_training = has_rating & has_description\n",
                "\n",
                "print(f\"\\nData Coverage:\")\n",
                "print(f\"  - Problems with ratings: {has_rating.sum()}\")\n",
                "print(f\"  - Problems with descriptions: {has_description.sum()}\")\n",
                "print(f\"  - Usable for training (both): {usable_for_training.sum()}\")\n",
                "print(f\"  - Missing ratings (targets): {(~has_rating & has_description).sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EDA: Rating distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "# Histogram\n",
                "axes[0].hist(df_merged[has_rating]['Rating'], bins=30, edgecolor='black', alpha=0.7)\n",
                "axes[0].set_xlabel('Rating')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].set_title('Rating Distribution')\n",
                "axes[0].axvline(df_merged[has_rating]['Rating'].mean(), color='red', linestyle='--', label='Mean')\n",
                "axes[0].legend()\n",
                "\n",
                "# Box plot by difficulty\n",
                "df_with_rating = df_merged[has_rating].copy()\n",
                "df_with_rating['difficulty_label'] = df_with_rating['difficulty'].fillna('Unknown')\n",
                "df_with_rating.boxplot(column='Rating', by='difficulty_label', ax=axes[1])\n",
                "axes[1].set_xlabel('Difficulty')\n",
                "axes[1].set_ylabel('Rating')\n",
                "axes[1].set_title('Rating by Difficulty')\n",
                "plt.suptitle('')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nRating Stats:\")\n",
                "print(df_merged[has_rating]['Rating'].describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter to problems with both rating and description\n",
                "df_train_pool = df_merged[usable_for_training].copy()\n",
                "print(f\"Training pool: {len(df_train_pool)} problems\\n\")\n",
                "\n",
                "# 4.1 Text Features: TF-IDF on descriptions\n",
                "print(\"Generating TF-IDF features...\")\n",
                "tfidf = TfidfVectorizer(max_features=500, stop_words='english', min_df=2)\n",
                "tfidf_matrix = tfidf.fit_transform(df_train_pool['description'].fillna(''))\n",
                "print(f\"   TF-IDF shape: {tfidf_matrix.shape}\")\n",
                "\n",
                "# 4.2 Categorical Features: One-hot encode tags\n",
                "print(\"Encoding topic tags...\")\n",
                "# Extract tags from topicTags (list of dicts with 'name' key)\n",
                "def extract_tags(tags):\n",
                "    if pd.isna(tags) or not isinstance(tags, list):\n",
                "        return []\n",
                "    return [tag.get('name', tag) if isinstance(tag, dict) else tag for tag in tags]\n",
                "\n",
                "df_train_pool['tag_list'] = df_train_pool['topicTags'].apply(extract_tags)\n",
                "mlb = MultiLabelBinarizer()\n",
                "tags_matrix = mlb.fit_transform(df_train_pool['tag_list'])\n",
                "print(f\"   Tags shape: {tags_matrix.shape} ({len(mlb.classes_)} unique tags)\")\n",
                "\n",
                "# 4.3 Numerical Features: acRate, totalAccepted (if available)\n",
                "print(\"Processing numerical features...\")\n",
                "numerical_features = []\n",
                "if 'acRate' in df_train_pool.columns:\n",
                "    numerical_features.append('acRate')\n",
                "# Note: totalAccepted not in lcid.json, skip if missing\n",
                "\n",
                "if numerical_features:\n",
                "    scaler = StandardScaler()\n",
                "    numerical_matrix = scaler.fit_transform(df_train_pool[numerical_features].fillna(0))\n",
                "    print(f\"   Numerical shape: {numerical_matrix.shape}\")\n",
                "else:\n",
                "    numerical_matrix = np.zeros((len(df_train_pool), 0))\n",
                "    scaler = None\n",
                "    print(f\"   No numerical features available\")\n",
                "\n",
                "print(\"\\nFeature engineering complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine features for baseline model\n",
                "from scipy.sparse import hstack, csr_matrix\n",
                "\n",
                "X_baseline = hstack([\n",
                "    tfidf_matrix,\n",
                "    csr_matrix(tags_matrix),\n",
                "    csr_matrix(numerical_matrix)\n",
                "])\n",
                "\n",
                "y = df_train_pool['Rating'].values\n",
                "\n",
                "print(f\"Baseline feature matrix: {X_baseline.shape}\")\n",
                "print(f\"Target vector: {y.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Baseline Model (TF-IDF + Tags + Numerical)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_baseline, y, test_size=0.2, random_state=RANDOM_SEED\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} problems\")\n",
                "print(f\"Validation set: {X_val.shape[0]} problems\")\n",
                "\n",
                "# Train Gradient Boosting Regressor\n",
                "print(\"\\nTraining baseline model...\")\n",
                "model_baseline = GradientBoostingRegressor(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    random_state=RANDOM_SEED,\n",
                "    verbose=0\n",
                ")\n",
                "model_baseline.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate\n",
                "y_pred_baseline = model_baseline.predict(X_val)\n",
                "rmse_baseline = np.sqrt(mean_squared_error(y_val, y_pred_baseline))\n",
                "mae_baseline = mean_absolute_error(y_val, y_pred_baseline)\n",
                "r2_baseline = r2_score(y_val, y_pred_baseline)\n",
                "\n",
                "print(f\"\\nBaseline Model Performance:\")\n",
                "print(f\"   RMSE: {rmse_baseline:.2f}\")\n",
                "print(f\"   MAE:  {mae_baseline:.2f}\")\n",
                "print(f\"   R²:   {r2_baseline:.3f}\")\n",
                "\n",
                "# Success criteria check\n",
                "if rmse_baseline < 100:\n",
                "    print(\"   RMSE < 100 (target met)\")\n",
                "else:\n",
                "    print(f\"   RMSE > 100 (target: <100)\")\n",
                "\n",
                "if r2_baseline > 0.75:\n",
                "    print(\"   R² > 0.75 (target met)\")\n",
                "else:\n",
                "    print(f\"   R² < 0.75 (target: >0.75)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Enhanced Model (+ Embeddings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate embeddings using SentenceTransformer\n",
                "print(\"Loading SentenceTransformer model (this may take a moment)...\")\n",
                "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "print(\"   Model loaded. Generating embeddings...\")\n",
                "\n",
                "descriptions = df_train_pool['description'].fillna('').tolist()\n",
                "embeddings = embedding_model.encode(descriptions, show_progress_bar=True, batch_size=32)\n",
                "print(f\"   Embeddings shape: {embeddings.shape}\")\n",
                "\n",
                "# Combine baseline features + embeddings\n",
                "X_enhanced = hstack([\n",
                "    X_baseline,\n",
                "    csr_matrix(embeddings)\n",
                "])\n",
                "\n",
                "print(f\"\\nEnhanced feature matrix: {X_enhanced.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split for enhanced model\n",
                "X_train_enh, X_val_enh, y_train_enh, y_val_enh = train_test_split(\n",
                "    X_enhanced, y, test_size=0.2, random_state=RANDOM_SEED\n",
                ")\n",
                "\n",
                "# Train enhanced model\n",
                "print(\"Training enhanced model...\")\n",
                "model_enhanced = GradientBoostingRegressor(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    random_state=RANDOM_SEED,\n",
                "    verbose=0\n",
                ")\n",
                "model_enhanced.fit(X_train_enh, y_train_enh)\n",
                "\n",
                "# Evaluate\n",
                "y_pred_enhanced = model_enhanced.predict(X_val_enh)\n",
                "rmse_enhanced = np.sqrt(mean_squared_error(y_val_enh, y_pred_enhanced))\n",
                "mae_enhanced = mean_absolute_error(y_val_enh, y_pred_enhanced)\n",
                "r2_enhanced = r2_score(y_val_enh, y_pred_enhanced)\n",
                "\n",
                "print(f\"\\nEnhanced Model Performance:\")\n",
                "print(f\"   RMSE: {rmse_enhanced:.2f}\")\n",
                "print(f\"   MAE:  {mae_enhanced:.2f}\")\n",
                "print(f\"   R²:   {r2_enhanced:.3f}\")\n",
                "\n",
                "# Compare with baseline\n",
                "print(f\"\\nComparison (Baseline → Enhanced):\")\n",
                "print(f\"   RMSE: {rmse_baseline:.2f} → {rmse_enhanced:.2f} ({rmse_enhanced - rmse_baseline:+.2f})\")\n",
                "print(f\"   MAE:  {mae_baseline:.2f} → {mae_enhanced:.2f} ({mae_enhanced - mae_baseline:+.2f})\")\n",
                "print(f\"   R²:   {r2_baseline:.3f} → {r2_enhanced:.3f} ({r2_enhanced - r2_baseline:+.3f})\")\n",
                "\n",
                "# Success criteria\n",
                "if rmse_enhanced < 100:\n",
                "    print(\"   RMSE < 100 (target met)\")\n",
                "if mae_enhanced < 75:\n",
                "    print(\"   MAE < 75 (target met)\")\n",
                "if r2_enhanced > 0.75:\n",
                "    print(\"   R² > 0.75 (target met)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Prediction & Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify problems missing ratings\n",
                "df_missing = df_merged[~has_rating & has_description].copy()\n",
                "print(f\"Problems missing ratings: {len(df_missing)}\")\n",
                "\n",
                "if len(df_missing) > 0:\n",
                "    # Generate features for missing problems\n",
                "    print(\"\\nGenerating predictions for missing ratings...\")\n",
                "    \n",
                "    # TF-IDF\n",
                "    tfidf_missing = tfidf.transform(df_missing['description'].fillna(''))\n",
                "    \n",
                "    # Tags\n",
                "    df_missing['tag_list'] = df_missing['topicTags'].apply(extract_tags)\n",
                "    tags_missing = mlb.transform(df_missing['tag_list'])\n",
                "    \n",
                "    # Numerical\n",
                "    if numerical_features and scaler:\n",
                "        numerical_missing = scaler.transform(df_missing[numerical_features].fillna(0))\n",
                "    else:\n",
                "        numerical_missing = np.zeros((len(df_missing), 0))\n",
                "    \n",
                "    # Embeddings\n",
                "    embeddings_missing = embedding_model.encode(\n",
                "        df_missing['description'].fillna('').tolist(),\n",
                "        show_progress_bar=True,\n",
                "        batch_size=32\n",
                "    )\n",
                "    \n",
                "    # Combine features\n",
                "    X_missing = hstack([\n",
                "        tfidf_missing,\n",
                "        csr_matrix(tags_missing),\n",
                "        csr_matrix(numerical_missing),\n",
                "        csr_matrix(embeddings_missing)\n",
                "    ])\n",
                "    \n",
                "    # Predict using enhanced model\n",
                "    predicted_ratings = model_enhanced.predict(X_missing)\n",
                "    df_missing['predicted_rating'] = predicted_ratings\n",
                "    \n",
                "    print(f\"\\nPredictions generated\")\n",
                "    print(f\"\\nPredicted Rating Stats:\")\n",
                "    print(df_missing['predicted_rating'].describe())\n",
                "    \n",
                "    # Show sample predictions\n",
                "    print(f\"\\nSample Predictions:\")\n",
                "    sample_cols = ['Title', 'TitleSlug', 'difficulty', 'predicted_rating']\n",
                "    available_cols = [c for c in sample_cols if c in df_missing.columns]\n",
                "    print(df_missing[available_cols].head(10).to_string(index=False))\n",
                "else:\n",
                "    print(\"No problems missing ratings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save augmented dataset\n",
                "if len(df_missing) > 0:\n",
                "    # Merge predictions back into main dataset\n",
                "    df_merged['predicted_rating'] = np.nan\n",
                "    df_merged.loc[df_missing.index, 'predicted_rating'] = df_missing['predicted_rating']\n",
                "    \n",
                "    # Create final rating column (use actual if available, else predicted)\n",
                "    df_merged['final_rating'] = df_merged['Rating'].fillna(df_merged['predicted_rating'])\n",
                "    \n",
                "    # Save to JSON\n",
                "    output_file = 'generated/merged_problems_with_ratings.json'\n",
                "    \n",
                "    # Convert to dict format similar to merged_problems.json\n",
                "    output_data = {\n",
                "        'questions': df_merged.to_dict(orient='records')\n",
                "    }\n",
                "    \n",
                "    with open(output_file, 'w') as f:\n",
                "        json.dump(output_data, f, indent=2)\n",
                "    \n",
                "    print(f\"Saved augmented dataset to: {output_file}\")\n",
                "    print(f\"   Total problems: {len(df_merged)}\")\n",
                "    print(f\"   With actual ratings: {df_merged['Rating'].notna().sum()}\")\n",
                "    print(f\"   With predicted ratings: {df_merged['predicted_rating'].notna().sum()}\")\n",
                "    print(f\"   Total with ratings (actual or predicted): {df_merged['final_rating'].notna().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8.1 Predicted vs Actual (Scatter Plot)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Baseline model\n",
                "axes[0].scatter(y_val, y_pred_baseline, alpha=0.5, s=20)\n",
                "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect prediction')\n",
                "axes[0].set_xlabel('Actual Rating')\n",
                "axes[0].set_ylabel('Predicted Rating')\n",
                "axes[0].set_title(f'Baseline Model (RMSE: {rmse_baseline:.2f}, R²: {r2_baseline:.3f})')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Enhanced model\n",
                "axes[1].scatter(y_val_enh, y_pred_enhanced, alpha=0.5, s=20, color='green')\n",
                "axes[1].plot([y_val_enh.min(), y_val_enh.max()], [y_val_enh.min(), y_val_enh.max()], 'r--', lw=2, label='Perfect prediction')\n",
                "axes[1].set_xlabel('Actual Rating')\n",
                "axes[1].set_ylabel('Predicted Rating')\n",
                "axes[1].set_title(f'Enhanced Model (RMSE: {rmse_enhanced:.2f}, R²: {r2_enhanced:.3f})')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8.2 Residual Plot\n",
                "residuals_baseline = y_val - y_pred_baseline\n",
                "residuals_enhanced = y_val_enh - y_pred_enhanced\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Baseline\n",
                "axes[0].scatter(y_pred_baseline, residuals_baseline, alpha=0.5, s=20)\n",
                "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
                "axes[0].set_xlabel('Predicted Rating')\n",
                "axes[0].set_ylabel('Residual (Actual - Predicted)')\n",
                "axes[0].set_title('Baseline Model Residuals')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Enhanced\n",
                "axes[1].scatter(y_pred_enhanced, residuals_enhanced, alpha=0.5, s=20, color='green')\n",
                "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
                "axes[1].set_xlabel('Predicted Rating')\n",
                "axes[1].set_ylabel('Residual (Actual - Predicted)')\n",
                "axes[1].set_title('Enhanced Model Residuals')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Residual mean (should be ~0):\")\n",
                "print(f\"  Baseline: {residuals_baseline.mean():.2f}\")\n",
                "print(f\"  Enhanced: {residuals_enhanced.mean():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8.3 Rating Distribution Comparison\n",
                "if len(df_missing) > 0:\n",
                "    fig, ax = plt.subplots(figsize=(12, 5))\n",
                "    \n",
                "    ax.hist(df_merged[has_rating]['Rating'], bins=30, alpha=0.5, label='Known Ratings', edgecolor='black')\n",
                "    ax.hist(df_missing['predicted_rating'], bins=30, alpha=0.5, label='Predicted Ratings', edgecolor='black')\n",
                "    ax.set_xlabel('Rating')\n",
                "    ax.set_ylabel('Frequency')\n",
                "    ax.set_title('Distribution: Known vs Predicted Ratings')\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No predicted ratings to visualize.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "Implementation complete.\n",
                "\n",
                "**Model Performance:**\n",
                "- Baseline (TF-IDF + Tags): See metrics above\n",
                "- Enhanced (+ Embeddings): See metrics above\n",
                "\n",
                "**Next Steps:**\n",
                "1. Review the scatter plots - predictions should cluster near the diagonal\n",
                "2. Check residual plots - errors should be randomly distributed around 0\n",
                "3. Verify predicted ratings align with difficulty labels\n",
                "4. Use `generated/merged_problems_with_ratings.json` in your application"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}